{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>kache is a modern cloud-native web accelerator and HTTP caching proxy that is highly available, reliable, and performant. It supports the latest RFC specifications and is capable of handling high traffic loads, scaling easily, and supporting distributed caching systems.</p>"},{"location":"getting-started/","title":"Getting started","text":""},{"location":"license/","title":"License","text":"<p>MIT License</p> <p>Copyright (c) 2023 kache.io</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"support/","title":"Sponsors","text":"<p>kache is sponsored and supported by Media Tech Lab.</p> <p> </p>"},{"location":"blog/","title":"Blog","text":""},{"location":"intro/configuration/","title":"Configuration","text":"<p>Kache can be easily configured with a YAML-based configuration file. </p> <p>The configuration contains everything that defines how HTTP requests and responses are handled and cached. Elements of the configuration include defining and configuring listeners as endpoints to which downstream clients can connect and upstream target hosts to which Kache connects.</p> <p>The HTTP caching element contains the configuration options for the HTTP caching filter or middleware that sits between listeners and upstream targets, and the provider element that configures the actual storage backend.</p> <p>Other elements allow to configure operations such as logging and exposing the Admin API.</p> <p>For a full overview, check the configuration reference or the sample configuration file on GitHub:</p> <ul> <li>Reference </li> <li>Sample config</li> </ul>"},{"location":"intro/installation/","title":"Installation","text":"<p>You can install and run Kache in the following ways:</p> <ul> <li>Use the official Docker image</li> <li>Use the binary distribution</li> <li>Build binary from source</li> <li>Use the Helm Chart</li> </ul>"},{"location":"intro/installation/#use-the-official-docker-image","title":"Use the Official Docker Image","text":"<p>Use one of the official Docker images and run it with the sample configuration file:</p> <pre><code>docker run -d -p 8080:8080 -p 80:80 \\\n    -v $PWD/kache.yml:/etc/kache/kache.yml \\\n    kache:latest -config.file=/etc/kache/kache.yml \n</code></pre>"},{"location":"intro/installation/#use-the-binary-distribution","title":"Use the binary distribution","text":"<p>To run kache, get the latest binary from the releases page and run it with the sample configuration file:</p> <pre><code>./kache -config.file=kache.yml\n</code></pre>"},{"location":"intro/installation/#build-binary-from-source","title":"Build binary from source","text":"<pre><code>git clone https://github.com/kacheio/kache\n</code></pre>"},{"location":"intro/installation/#use-the-helm-chart","title":"Use the Helm Chart","text":"<p>Info</p> <p>Comming soon!</p>"},{"location":"intro/installation/#quick-start","title":"Quick Start","text":"<p>If you want to run kache with a distributed caching backend (e.g. Redis), you can use and run this example docker-compose as a starting point:</p> <pre><code>docker-compose -f cloud/docker-compose.yml up \n</code></pre> <p>Tip</p> <p>Check the Quick Starts for Docker and Kubernetes to learn more.</p>"},{"location":"intro/quick-start-k8s/","title":"Kache on Kubernetes","text":"<p>The following describes how to run kache on a local Kubernetes cluster.</p> <p>Warning</p> <p>Please note that this is not intended for use in a production environment. We will provide more sophisticated configurations for operations in the future. </p>"},{"location":"intro/quick-start-k8s/#start-a-kubernetes-cluster","title":"Start a Kubernetes cluster","text":"<p>To start a cluster, use minikube or Docker Desktop.</p> <pre><code>minikube start\n</code></pre>"},{"location":"intro/quick-start-k8s/#deploy-configmap","title":"Deploy ConfigMap","text":"<p>Create a ConfigMap that contains the kache configuration:</p> <pre><code>kubectl create configmap kache-config --from-file=cloud/kubernetes/configmap.yml </code></pre> <p>Apply the ConfigMap:</p> <pre><code>kubectl apply -f cloud/kubernetes/configmap.yml\n</code></pre> configmap.yml Kubernetes <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\nname: kache-config\ndata:\nconfig.yml: |-\nlisteners:\nweb1:\naddr: :80\nweb2:\naddr: :1337\nupstreams:\n- name: service1\naddr: \"http://localhost:8000\"\npath: \"/service/1\"\n- name: service2\naddr: \"http://example.com\"\npath: \"/\"\napi:\nport: 1338\ndebug: true\nlogging:\nlevel: debug\nprovider:\nbackend: redis\nredis:\nendpoint: \"redis-master:6379\"\nusername:\npassword:\ndb:\n</code></pre>"},{"location":"intro/quick-start-k8s/#deploy-redis","title":"Deploy Redis","text":"<pre><code>kubectl apply -f cloud/kubernetes/redis-master.yml\n</code></pre> redis.yml Kubernetes <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: redis-master\nlabels:\napp: redis\nspec:\nselector:\nmatchLabels:\napp: redis\nrole: master\ntier: backend\nreplicas: 1\ntemplate:\nmetadata:\nlabels:\napp: redis\nrole: master\ntier: backend\nspec:\ncontainers:\n- name: master\nimage: redis\nresources:\nrequests:\ncpu: 100m\nmemory: 100Mi\nports:\n- containerPort: 6379\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: redis-master\nlabels:\napp: redis\nrole: master\ntier: backend\nspec:\nports:\n- port: 6379\ntargetPort: 6379\nselector:\napp: redis\nrole: master\ntier: backend\n</code></pre>"},{"location":"intro/quick-start-k8s/#deploy-kache","title":"Deploy Kache","text":"<pre><code>kubectl apply -f cloud/kubernetes/kache.yml\n</code></pre> kache.yml Kubernetes <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: kache\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp: kache\ntemplate:\nmetadata:\nlabels:\napp: kache\nspec:\ncontainers:\n- name: kache\nimage: kacheio/kache:main\nimagePullPolicy: Always\nargs:\n- \"-config.file=/etc/kache/config.yml\"\nenv:\n- name: NAMESPACE\nvalueFrom:\nfieldRef:\nfieldPath: metadata.namespace\nvolumeMounts:\n- name: config\nmountPath: /etc/kache\nports:\n- containerPort: 8080\nname: http\n- containerPort: 1337\nname: web\n- containerPort: 1338\nname: api\nresources:\nrequests:\ncpu: 100m\nmemory: 100Mi\nvolumes:\n- name: config\nconfigMap:\nname: kache-config\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: kache-service\nlabels:\napp: kache\nspec:\ntype: LoadBalancer\nports:\n- name: \"http\"\nport: 80\ntargetPort: http\n- name: \"web\"\nport: 1337\ntargetPort: web\n- name: \"api\"\nport: 1338\ntargetPort: api\nselector:\napp: kache\n</code></pre>"},{"location":"intro/quick-start-k8s/#accessing-the-service","title":"Accessing the service","text":"<p>Check that the Pods are up and running:</p> <pre><code>$ kubectl get pods NAME                           READY   STATUS    RESTARTS   AGE\nkache-54cd8ffd96-xzdqg         1/1     Running   0          14h\nredis-master-d4f785667-mpmvg   1/1     Running   0          14h\n</code></pre> <p>The Kache service is exposed as a LoadBalancer via the service with mapped ports and is accessible on localhost.</p> <pre><code>$ kubectl get svc\n\nNAME            TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                                      AGE\nkache-service   LoadBalancer   10.110.92.73   localhost     80:30135/TCP,1337:32284/TCP,1338:30691/TCP   44h\nkubernetes      ClusterIP      10.96.0.1      &lt;none&gt;        443/TCP                                      44h\nredis-master    ClusterIP      10.97.188.34   &lt;none&gt;        6379/TCP                                     44h\n</code></pre> <p>Use the above endpoints to access the service:</p> <pre><code>curl http://127.0.0.1:1337/\n</code></pre> <p>Access the API:</p> <pre><code>curl http://127.0.0.1:1338/api/\n</code></pre>"},{"location":"intro/quick-start-k8s/#troubleshooting","title":"Troubleshooting","text":"<p>If there are problems loading the configuration, verify that the Pod has the latest configuration available:</p> <pre><code>kubectl exec $POD_NAME -- cat /etc/kache/config.yml </code></pre>"},{"location":"intro/quick-start/","title":"Quick Start","text":"<p>The following describes how to run kache locally with Docker.</p>"},{"location":"intro/quick-start/#set-up-a-docker-compose","title":"Set up a docker-compose","text":"<p>Create and define a docker-compose.yml and define a kache service that uses the official Kache image:</p> <pre><code>services:\nkache:\n# Use the `main` tag for the latest development image \n# or `latest` tag for the latest stable version.\nimage: kacheio/kache:main\ncontainer_name: kache\n# Start the container with the mounted config file.\ncommand:\n- \"-config.file=/etc/kache/kache.sample.yml\"\n# Expose ports that are configured in the kache config file.\nports:\n- \"80:80\"\n- \"8080:8080\"\n- \"1337:1337\"\n- \"1338:1338\"\n# Mount the config file.\nvolumes:\n- \"./../kache.sample.yml:/etc/kache/kache.sample.yml\"\n# Use redis as distributed remote caching backend.\nredis:\nimage: \"redis:alpine\"\n</code></pre> <p>That's all you need to run Kache with Redis as a distributed caching backend. </p>"},{"location":"intro/quick-start/#run-kache","title":"Run kache","text":"<p>Now start Kache with the following command:</p> <pre><code>docker-compose -f cloud/docker/docker-compose.yml up \n</code></pre>"},{"location":"intro/quick-start/#access-kache","title":"Access kache","text":"<p>You can now access the service under <code>http://localhost:8080/</code> and the API under <code>http://localhost:1338/api/cache/keys</code> which, for example, returns all the keys currently in the cache.</p>"},{"location":"reference/","title":"Configuration reference","text":"<p>This section gives an overview of all relevant configuration options and provides further details on the core concepts of Kache.</p>"},{"location":"reference/api/","title":"API","text":"<p>Kache offers a REST API into Kache, exposing endpoints for cache management and debug information.</p> <p>For example, the following configuration exposes the API on the specified port and activates endpoints exposing debug informations.</p> YAML <pre><code>api:\n# API port.\nport: 1338\n# Activate debug endpoints.\ndebug: true\n</code></pre> <p>The Kache API endpoints are generally available under path prefix <code>/api</code>, unless not specified a different prefix. A complete overview of the API and its endpoints is available here.</p>"},{"location":"reference/api/#reference","title":"Reference","text":"Directive Type Description <code>port</code> <code>string</code> The port the API is available at. <code>prefix</code> <code>string</code> Adds a custom path prefix. <code>debug</code> <code>bool</code> Activates debug endpoints."},{"location":"reference/api_specification/","title":"API","text":"<p>Kache provides an API for cache control and managment and exposes a number of informations through the API, such as the configuration of all listeners, targets, etc. The admin API can be enabled via the configuration.</p>"},{"location":"reference/api_specification/#security","title":"Security","text":"<p>Enabling the API will expose all configuration elements, including sensitive data.</p> <p>Kache supports the configuration of an access control list, which grants or denies access to API resources and objects. </p> <p>In production, it should be further protected by application level protection mechanisms, e.g., secured by authentication and authorization. Or, by transport level protection mechanisms such as NOT publicly exposing the API's port, but keeping it restricted to internal networks (principle of least privilege, applied to networks).</p>"},{"location":"reference/api_specification/#configuration","title":"Configuration","text":""},{"location":"reference/api_specification/#port","title":"Port","text":"<p>This enables the API and exposes its endpoints via the specified port.</p> YAML <pre><code>api:\n# API port.\nport: 1338\n</code></pre>"},{"location":"reference/api_specification/#prefix","title":"Prefix","text":"<p>Configure a custom path prefix for all API endpoints, other than debug.</p> YAML <pre><code>api:\n# Enable API at port.\nport: 1338\n# Customize path prefix, default is '/api'.\nprefix: \"/custom-api-prefix/\"\n</code></pre>"},{"location":"reference/api_specification/#acl","title":"ACL","text":"<p>Configure an access control list to grant access to resources only for the specified IP addresses. If  <code>acl</code> is specified, access is granted only for requests originating from IPs on the list. All others  will be denied. If the list is empty or the item is not specified in the configuration at all,  any request is allowed.</p> YAML <pre><code>api:\n# Enable API at port.\nport: 1338\n# Access control list. If empty or not specified at all, \n# any request is allowed to access API resources and objects.\nacl: \"127.0.0.1, 10.22.0.0\"\n</code></pre>"},{"location":"reference/api_specification/#debug","title":"Debug","text":"<p>Activate endpoints for general debug informations.</p> YAML <pre><code>api:\n# API port.\nport: 1338\n# Activate debug endpoints.\ndebug: true\n</code></pre>"},{"location":"reference/api_specification/#endpoints","title":"Endpoints","text":"Path Method Description <code>/api/cache/keys</code> <code>GET</code> Returns the keys currently in the cache. <code>/api/cache/keys/&lt;key&gt;</code> <code>GET</code> Returns the key info about <code>&lt;key&gt;</code>. <code>/api/cache/keys/purge?key=&lt;key&gt;</code> <code>GET</code> Purges a <code>&lt;key&gt;</code> from the cache <code>/api/version</code> <code>GET</code> Returns the Kache version info. <code>/debug/vars</code> <code>GET</code> See the expvar Go documentation. <code>/debug/pprof</code> <code>GET</code> See the pprof Index Go documentation. <code>/debug/pprof/cmdline</code> <code>GET</code> See the pprof Cmdline Go documentation. <code>/debug/pprof/profile</code> <code>GET</code> See the pprof Profile Go documentation. <code>/debug/pprof/symbol</code> <code>GET</code> See the pprof Symbol Go documentation. <code>/debug/pprof/trace</code> <code>GET</code> See the pprof Trace Go documentation."},{"location":"reference/cache/","title":"HTTP Cache","text":"<p>The HTTP cache acts as a filter or middleware sitting between listeners and upstream targets. All incoming requests are routed through the HTTP cache, which implements most of the complexities of HTTP caching semantics and relevant RFC specifications1.</p> <p>For HTTP Requests:</p> <ul> <li>Considers the Cache-Control header of the request. For example, if the request has <code>Cache-Control: no-store</code>, the request is not cached.</li> <li>Does not store HTTP HEAD requests.</li> </ul> <p>For HTTP Responses:</p> <ul> <li>Only caches responses with enough data to calculate the freshness lifetime2.</li> <li>Considers the Cache-Control header from the upstream host. For example, if the HTTP response returns with status code <code>200</code> and <code>Cache-Control: max-age=60</code>, the response will be cached.</li> <li>Only caches responses with following status codes:<code>200, 203, 204, 206, 300, 301, 308, 404, 405, 410, 414, 451, 501</code>.</li> </ul> <p>The actual storage of HTTP responses is delegated to the implementations of a caching provider. These implementations can cover different requirements such as persistence, performance, and distribution, from local RAM caches to globally distributed persistent caches. They can be fully custom caches or wrappers and adapters for local or remote open source or proprietary caches. Currently, the available cache storage implementations are In-memory and Redis. More informations and relevant configurations of caching providers can be foud here.</p>"},{"location":"reference/cache/#configuration","title":"Configuration","text":"<p>The following example configures the HTTP cache with a default TTL (time-to-live) of 1200s and activates a custom debug header (X-Header) attached to every response that is served from Kache. Debug headers are added to the request and contain information if the response was served from cache or not. They are presented in a canonical format where <code>x_header_name</code> specifies the name of the header entry with its value indicating, if it was a cache <code>HIT</code> or <code>MISS</code>.</p> YAML <pre><code>cache:\n# Activate debug header.\nx_header: true\n# Set debug header name to 'X-Kache'.\nx_header_name: x-kache\n# Set default ttl for single items.\ndefault_ttl: 1200s\n</code></pre>"},{"location":"reference/cache/#reference","title":"Reference","text":"Directive Type Description <code>x_header</code> <code>bool</code> Activates the X-Cache debug header. If set to <code>true</code>, Kache will add a HTTP response header to each response indicating if it was served from cache or not (cache hit or miss). <code>x_header_name</code> <code>string</code> Specifies the name of the X-Cache debug header. For example, if set to <code>x-kache</code> and in case of a cache hit, the response will contain an additional HTTP header with <code>X-Kache</code> as key and <code>HIT</code> as value. Default is 'X-Kache'. <code>default_ttl</code> <code>string</code> Is the default TTL (time-to-live) for cached items. The value is a duration string. A duration string is a possibly signed sequence of decimal numbers, each with optional fraction and a unit suffix, such as \"300ms\", \"1.5h\", or \"2h45m\". Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\". TTL must be greater than 0. <ol> <li> <p>RFC7234 -- HTTP Caching \u21a9</p> </li> <li> <p>RFC7234 -- Calculating freshness lifetime \u21a9</p> </li> </ol>"},{"location":"reference/file/","title":"File","text":"YAML <pre><code>listeners:\nweb1:\naddr: :80\nweb2:\naddr: :1337\nupstreams:\n- name: service1\naddr: \"http://localhost:8000\"\npath: \"/service/1\"\n- name: service2\naddr: \"http://example.com\"\napi:\nport: 1338\ndebug: true\nlogging:\nlevel: debug # trace, debug, info, warn, error, fatal, panic\ncolor: true\ncache:\nx_header: true\nx_header_name: x-kache\ndefault_ttl: 1200s\nprovider:\nbackend: redis\nlayered: true\nredis:\nendpoint: \"redis:6379\"\nusername:\npassword:\ndb:\nmax_item_size: 10000000\nmax_queue_concurrency: 56\nmax_queue_buffer_size: 24000\ninmemory:\nmax_size: 1000000000 max_item_size: 50000000\ndefault_ttl: 120s\n</code></pre>"},{"location":"reference/listeners/","title":"Listeners","text":"<p>A listener is a named network location (e.g., port, etc.) that can be connected to by downstream clients. Kache exposes one or more listeners that downstream hosts can connect to.</p> <p>A listener is configured with a unique name and a network adress. The address defines a port, and optionally a hostname, on which kache listens for incoming connections.</p>"},{"location":"reference/listeners/#configuration","title":"Configuration","text":"<p>For example, the following configuration exposes two named listeners, <code>web1</code> and <code>web2</code> listening on ports <code>:80</code> and <code>:1337</code>, respectively.</p> YAML <pre><code>listeners:\n# Listener 1 named web1\nweb1:\naddr: :80\n# Listener 2 named web2\nweb2:\naddr: :1337\n</code></pre> <p>After the listeners are up and running, kache accepts connections and forwards them to defined upstream targets.</p>"},{"location":"reference/listeners/#reference","title":"Reference","text":"Directive Type Description <code>&lt;name&gt;</code> <code>token</code> Name variable is the unique name of the listener. <code>addr</code> <code>string</code> The network location exposed by Kache that can be connected to by downstream clients. It accepts a port, and optionally a hostname, in the format of <code>[host]:port</code>."},{"location":"reference/logging/","title":"Logging","text":"<p>Kache supports efficient structured logging with different output locations and formats:</p> <ul> <li>Console</li> <li>Log file</li> <li>JSON</li> </ul>"},{"location":"reference/logging/#console","title":"Console","text":"<p>Pretty logging to the console is supported and activated by default.</p> YAML <pre><code>logging:\n# Set level to debug.\nlevel: debug\n# Activate color log output.\ncolor: true # disabled by default\n</code></pre>"},{"location":"reference/logging/#file","title":"File","text":"<p>To use structured file-based logging, just configure a log location.</p> YAML <pre><code>logging:\nlevel: info\n# Activate file-based logging.\nfile: /var/log/kache/kache.log\n# Configure file-based logging.\nMaxSize:    500 # 500 megabytes\nMaxBackups: 3 MaxAge:     28 # days\nCompress:   true # disabled by default\n</code></pre> Info <p>Kache opens or creates the logfile on first write. If the file exists and is less than MaxSize megabytes, it will open and append to that file. If the file exists and its size is &gt;= MaxSize megabytes, the file is renamed by putting the current time in a timestamp in the name immediately before the file's extension (or the end of the filename if there's no extension). A new log file is then created using original filename.</p> <p>Whenever a write would cause the current log file exceed MaxSize megabytes, the current file is closed, renamed, and a new log file created with the original name. Thus, the filename you give Logger is always the \"current\" log file.</p> <p>Backups use the log file name given to Logger, in the form name-timestamp.ext where name is the filename without the extension, timestamp is the time at which the log was rotated formatted with the time.Time format of 2006-01-02T15-04-05.000 and the extension is the original extension. For example, if your Logger.Filename is /var/log/foo/server.log, a backup created at 6:30pm on Nov 11 2016 would use the filename /var/log/foo/server-2016-11-04T18-30-00.000.log</p> <p>More..</p>"},{"location":"reference/logging/#json","title":"JSON","text":"<p>To log in JSON format, just change the format of the log output to <code>json</code>. This works with both, console and file logging.</p> YAML <pre><code>logging:\n# Set level to debug.\nlevel: debug\n# Set format to json.\nformat: json\n</code></pre>"},{"location":"reference/logging/#reference","title":"Reference","text":"Directive Type Description <code>level</code> <code>string</code> Specifies the log level. Supported log levels are: <code>trace</code>, <code>debug</code>, <code>info</code>, <code>warn</code>, <code>error</code>, <code>fatal</code>, <code>panic</code>. <code>format</code> <code>string</code> Specifies the log format. Default is console, <code>json</code> changes log output to JSON format. <code>color</code> <code>bool</code> Enables colored log output. Disabled by default. Logging to files never colors the output. <code>file</code> <code>string</code> Sets the log file path. <code>max_size</code> <code>int</code> Sets the maximum size in megabytes of the log file before it gets rotated. It defaults to 100 megabytes. <code>max_age</code> <code>int</code> Sets the maximum number of days to retain old log files based on the timestamp encoded in their filename.  Note that a day is defined as 24 hours and may not exactly correspond to calendar days due to daylight savings, leap seconds, etc. The default is not to remove old log files based on age. <code>max_backups</code> <code>int</code> Sets the maximum number of old log files to retain.  The default `s to retain all old log files (though MaxAge may still cause them to get deleted.) <code>compress</code> <code>bool</code> Specifies if the rotated log files should be compressed using gzip. The default is not to perform compression."},{"location":"reference/provider/","title":"Provider","text":"<p>A provider is responsible for the actual storage of HTTP responses. A provider implementation can satisfy different requirements such as persistence, performance, and distribution, from local RAM caches to globally distributed persistent caches. They can be fully custom caches or wrappers and adapters for local or remote open-source or proprietary caches. </p> <p>At the moment, the available and supported cache storage implementations are In-memory and Redis.</p> <ul> <li>In-memory</li> <li>Remote</li> </ul>"},{"location":"reference/provider/#in-memory-cache","title":"In-Memory cache","text":"<p>In-memory stores HTTP responses in a local in-memory cache. It is a volatile storage and does not persist the entries to disk. It is also not shared between multiple Kache instances. The in-memory cache is size-bound and ensures the total cache size approximately does not exceed the configured <code>max_size</code> in bytes. The following shows an example configuration of an in-memory cache with maximum overall cache size and maximum single item size constraints and with TTL eviction enabled and set to 120s.</p> YAML <pre><code>provider:\n# Activate inmemory as cache backend.\nbackend: inmemory\n# Configure inmemory cache.\ninmemory:\n# Overall cache size of 1GB.\nmax_size: 1000000000\n# Max item size of 50MB.\nmax_item_size: 50000000\n# Items expire after 120s.\ndefault_ttl: 120s\n</code></pre> <p>Tip</p> <p>To disable TTL eviction, set <code>default_ttl</code> to -1.</p>"},{"location":"reference/provider/#remote-cache","title":"Remote cache","text":"<p>A remote cache is a distributed remote cache such as Redis or Memcached. The following example configures Redis as a remote caching provider.</p> YAML <pre><code>provider:\n# Activate redis as cache backend.\nbackend: redis\n# Configure redis remote cache.\nredis:\nendpoint: \"redis:6379\" username:\npassword:\ndb: max_item_size: 50000000\nmax_queue_concurrency: 56\nmax_queue_buffer_size: 24000\n</code></pre>"},{"location":"reference/provider/#layered-cache","title":"Layered cache","text":"<p>A layered or tiered cache adds a local in-memory caching layer on top of a remote cache provider (typically a distributed and shared network cache). Items will always be stored in both caches. Fetches are only satified by the underlying remote cache, if the item does not exist in the local cache. The local cache will remove items, depending on the capacity constraints of the cache or the lifetime constraints of the cached item, respectively. This strategy can be used to speed up operations when working with remote network caches.</p> <p>The following example configures a remote cache that is layered by a local in-memory cache.</p> YAML <pre><code>provider:\n# Activate redis as the main remote cache backend.\nbackend: redis\n# Activate layered caching strategy.\nlayered: true\n// Configure remote redis cache (layer 2).\nredis:\nendpoint: \"redis:6379\"\nusername:\npassword:\ndb:\nmax_item_size: 50000000\nmax_queue_concurrency: 56\nmax_queue_buffer_size: 24000\n// Configure local inmemory cache (layer 1).\ninmemory:\nmax_size: 1000000000\nmax_item_size: 50000000\ndefault_ttl: 120s\n</code></pre>"},{"location":"reference/provider/#reference","title":"Reference","text":""},{"location":"reference/provider/#general","title":"General","text":"Directive Type Description <code>backend</code> <code>string</code> Specifies the backend used as the underlying cache storage provider. Valid values are <code>inmemory</code> for a local in-memory cache and <code>redis</code> for the distributed remote cache based on Redis. <code>layered</code> <code>bool</code> Activates a two-tier cache strategy, putting a local cache layer on top of the remote cache to speed up operations."},{"location":"reference/provider/#in-memory-cache_1","title":"In-Memory cache","text":"Directive Type Description <code>max_size</code> <code>int</code> Sets the overall maximum number of bytes the cache can hold. <code>max_item_size</code> <code>int</code> Sets the maximum size of a single item. <code>default_ttl</code> <code>string</code> Sets the defautl TTL of a single item. The value is a duration string. A duration string is a possibly signed sequence of decimal numbers, each with optional fraction and a unit suffix, such as \"300ms\", \"1.5h\", or \"2h45m\". Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\". A valid TTL must be greater than 0. If TTL is set to -1, TTL eviction is disabled."},{"location":"reference/provider/#redis-remote-cache","title":"Redis remote cache","text":"Directive Type Description <code>endpoint</code> <code>string</code> Specifies the endpoint addresses of the Redis server. Either a single address or a comma-separated list of host:port addresses of cluster/sentinel nodes. <code>username</code> <code>string</code> Sets the username to authenticate the current connection with one of the connections defined in the ACL list when connecting to a Redis 6.0 instance, or greater, that is using the Redis ACL system. <code>password</code> <code>string</code> Optional password. Must match the password specified in the requirepass server configuration option (if connecting to a Redis 5.0 instance, or lower), or the User Password when connecting to a Redis 6.0 instance, or greater, that is using the Redis ACL system. <code>db</code> <code>int</code> Sets the database to be selected after connecting to the server. Default is <code>0</code>. <code>max_item_size</code> <code>int</code> Specifies the maximum size of an item stored in Redis. Items bigger than MaxItemSize are skipped. If set to 0, no maximum size is enforced. <code>max_queue_concurrency</code> <code>int</code> Specifies the maximum number of enqueued job operations allowed. <code>max_queue_buffer_size</code> <code>int</code> Specifies the maximum number of concurrent asynchronous job operations."},{"location":"reference/targets/","title":"Targets","text":"<p>Targets are a group of logically similar upstream hosts that Kache connects to. An upstream host receives connections and requests from Kache and returns responses.</p>"},{"location":"reference/targets/#configuration","title":"Configuration","text":"<p>The following example defines two upstream hosts identified by a unique name. The upstream target's address (<code>addr</code>) is a network location, Kache can connect and send requests to.</p> YAML <pre><code>upstreams:\n# Upstream service 1\n- name: service1\naddr: \"&lt;ip-service-1&gt;:&lt;port-service-1&gt;\"\npath: \"path/to/service/1\"\n# Upstream service 2\n- name: service2\naddr: \"&lt;ip-service-2&gt;:&lt;port-service-2&gt;\"\n</code></pre>"},{"location":"reference/targets/#reference","title":"Reference","text":"Directive Type Description <code>name</code> <code>string</code> The unique identifier of the upstream target. <code>addr</code> <code>string</code> The network location Kache can connect and send requests to. Typically a valid URL consisting of the private IP and Port of an upstream service. <code>path</code> <code>string</code> The path prefix used to match the upstream target. Path is forwarded as-is, hence the service is expected to listen on the specified path."},{"location":"setup/","title":"Setup","text":""}]}