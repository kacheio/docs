{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>kache is a modern cloud-native web accelerator and HTTP cache proxy that is highly available, reliable, and performant. It supports the latest RFC specifications and should be able to handle high traffic loads, be easily scalable, and support distributed caching systems.</p>"},{"location":"getting-started/","title":"Getting started","text":""},{"location":"license/","title":"License","text":"<p>MIT License</p> <p>Copyright (c) 2023 kache.io</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"support/","title":"Sponsors","text":"<p>kache is sponsored and supported by Media Tech Lab.</p> <p> </p>"},{"location":"blog/","title":"Blog","text":""},{"location":"intro/installation/","title":"Installation","text":"<p>You can install and run Kache in the following ways:</p> <ul> <li>Use the official Docker image</li> <li>Use the binary distribution</li> <li>Build binary from source</li> <li>Use the Helm Chart</li> </ul>"},{"location":"intro/installation/#use-the-official-docker-image","title":"Use the Official Docker Image","text":"<p>Use one of the official Docker images and run it with the sample configuration file:</p> <pre><code>docker run -d -p 8080:8080 -p 80:80 \\\n    -v $PWD/kache.yml:/etc/kache/kache.yml \\\n    kache:latest -config.file=/etc/kache/kache.yml \n</code></pre>"},{"location":"intro/installation/#use-the-binary-distribution","title":"Use the binary distribution","text":"<p>To run kache, get the latest binary from the releases page and run it with the sample configuration file:</p> <pre><code>./kache -config.file=kache.yml\n</code></pre>"},{"location":"intro/installation/#build-binary-from-source","title":"Build binary from source","text":"<pre><code>git clone https://github.com/kacheio/kache\n</code></pre>"},{"location":"intro/installation/#use-the-helm-chart","title":"Use the Helm Chart","text":"<p>Info</p> <p>Comming soon!</p>"},{"location":"intro/installation/#quick-start","title":"Quick Start","text":"<p>If you want to run kache with a distributed caching backend (e.g. Redis), you can use and run this example docker-compose as a starting point:</p> <pre><code>docker-compose -f cloud/docker-compose.yml up \n</code></pre> <p>Tip</p> <p>Check the Quick Starts for Docker and Kubernetes to learn more.</p>"},{"location":"intro/quick-start-k8s/","title":"Kache on Kubernetes","text":"<p>The following describes how to run kache on a local Kubernetes cluster.</p> <p>Warning</p> <p>Please note that this is not intended for use in a production environment. We will provide more sophisticated configurations for operations in the future. </p>"},{"location":"intro/quick-start-k8s/#start-a-kubernetes-cluster","title":"Start a Kubernetes cluster","text":"<p>To start a cluster, use minikube or Docker Desktop.</p> <pre><code>minikube start\n</code></pre>"},{"location":"intro/quick-start-k8s/#deploy-configmap","title":"Deploy ConfigMap","text":"<p>Create a ConfigMap that contains the kache configuration:</p> <pre><code>kubectl create configmap kache-config --from-file=cloud/kubernetes/configmap.yml </code></pre> <p>Apply the ConfigMap:</p> <pre><code>kubectl apply -f cloud/kubernetes/configmap.yml\n</code></pre> configmap.yml Kubernetes <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\nname: kache-config\ndata:\nconfig.yml: |-\nlisteners:\nweb1:\naddr: :80\nweb2:\naddr: :1337\nupstreams:\n- name: service1\naddr: \"http://localhost:8000\"\npath: \"/service/1\"\n- name: service2\naddr: \"http://example.com\"\npath: \"/\"\napi:\nport: 1338\ndebug: true\nlogging:\nlevel: debug\nprovider:\nbackend: redis\nredis:\nendpoint: \"redis-master:6379\"\nusername:\npassword:\ndb:\n</code></pre>"},{"location":"intro/quick-start-k8s/#deploy-redis","title":"Deploy Redis","text":"<pre><code>kubectl apply -f cloud/kubernetes/redis-master.yml\n</code></pre> redis.yml Kubernetes <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: redis-master\nlabels:\napp: redis\nspec:\nselector:\nmatchLabels:\napp: redis\nrole: master\ntier: backend\nreplicas: 1\ntemplate:\nmetadata:\nlabels:\napp: redis\nrole: master\ntier: backend\nspec:\ncontainers:\n- name: master\nimage: redis\nresources:\nrequests:\ncpu: 100m\nmemory: 100Mi\nports:\n- containerPort: 6379\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: redis-master\nlabels:\napp: redis\nrole: master\ntier: backend\nspec:\nports:\n- port: 6379\ntargetPort: 6379\nselector:\napp: redis\nrole: master\ntier: backend\n</code></pre>"},{"location":"intro/quick-start-k8s/#deploy-kache","title":"Deploy Kache","text":"<pre><code>kubectl apply -f cloud/kubernetes/kache.yml\n</code></pre> kache.yml Kubernetes <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: kache\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp: kache\ntemplate:\nmetadata:\nlabels:\napp: kache\nspec:\ncontainers:\n- name: kache\nimage: kacheio/kache:main\nimagePullPolicy: Always\nargs:\n- \"-config.file=/etc/kache/config.yml\"\nenv:\n- name: NAMESPACE\nvalueFrom:\nfieldRef:\nfieldPath: metadata.namespace\nvolumeMounts:\n- name: config\nmountPath: /etc/kache\nports:\n- containerPort: 8080\nname: http\n- containerPort: 1337\nname: web\n- containerPort: 1338\nname: api\nresources:\nrequests:\ncpu: 100m\nmemory: 100Mi\nvolumes:\n- name: config\nconfigMap:\nname: kache-config\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: kache-service\nlabels:\napp: kache\nspec:\ntype: LoadBalancer\nports:\n- name: \"http\"\nport: 80\ntargetPort: http\n- name: \"web\"\nport: 1337\ntargetPort: web\n- name: \"api\"\nport: 1338\ntargetPort: api\nselector:\napp: kache\n</code></pre>"},{"location":"intro/quick-start-k8s/#accessing-the-service","title":"Accessing the service","text":"<p>Check that the Pods are up and running:</p> <pre><code>$ kubectl get pods NAME                           READY   STATUS    RESTARTS   AGE\nkache-54cd8ffd96-xzdqg         1/1     Running   0          14h\nredis-master-d4f785667-mpmvg   1/1     Running   0          14h\n</code></pre> <p>The Kache service is exposed as a LoadBalancer via the service with mapped ports and is accessible on localhost.</p> <pre><code>$ kubectl get svc\n\nNAME            TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                                      AGE\nkache-service   LoadBalancer   10.110.92.73   localhost     80:30135/TCP,1337:32284/TCP,1338:30691/TCP   44h\nkubernetes      ClusterIP      10.96.0.1      &lt;none&gt;        443/TCP                                      44h\nredis-master    ClusterIP      10.97.188.34   &lt;none&gt;        6379/TCP                                     44h\n</code></pre> <p>Use the above endpoints to access the service:</p> <pre><code>curl http://127.0.0.1:1337/\n</code></pre> <p>Access the API:</p> <pre><code>curl http://127.0.0.1:1338/api/\n</code></pre>"},{"location":"intro/quick-start-k8s/#troubleshooting","title":"Troubleshooting","text":"<p>If there are problems loading the configuration, verify that the Pod has the latest configuration available:</p> <pre><code>kubectl exec $POD_NAME -- cat /etc/kache/config.yml </code></pre>"},{"location":"intro/quick-start/","title":"Quick Start","text":"<p>The following describes how to run kache locally with Docker.</p>"},{"location":"intro/quick-start/#set-up-a-docker-compose","title":"Set up a docker-compose","text":"<p>Create and define a docker-compose.yml and define a kache service that uses the official Kache image:</p> <pre><code>services:\nkache:\n# Use the `main` tag for the latest development image \n# or `latest` tag for the latest stable version.\nimage: kacheio/kache:main\ncontainer_name: kache\n# Start the container with the mounted config file.\ncommand:\n- \"-config.file=/etc/kache/kache.sample.yml\"\n# Expose ports that are configured in the kache config file.\nports:\n- \"80:80\"\n- \"8080:8080\"\n- \"1337:1337\"\n- \"1338:1338\"\n# Mount the config file.\nvolumes:\n- \"./../kache.sample.yml:/etc/kache/kache.sample.yml\"\n# Use redis as distributed remote caching backend.\nredis:\nimage: \"redis:alpine\"\n</code></pre> <p>That's all you need to run Kache with Redis as a distributed caching backend. </p>"},{"location":"intro/quick-start/#run-kache","title":"Run kache","text":"<p>Now start Kache with the following command:</p> <pre><code>docker-compose -f cloud/docker/docker-compose.yml up \n</code></pre>"},{"location":"intro/quick-start/#access-kache","title":"Access kache","text":"<p>You can now access the service under <code>http://localhost:8080/</code> and the API under <code>http://localhost:1338/api/cache/keys</code> which returns all the keys currentlz in the cache.</p>"},{"location":"reference/","title":"Reference","text":""},{"location":"reference/cache/","title":"HTTP Cache","text":"<p>The HTTP cache acts as a filter or middleware sitting between listeners and upstream targets. All incoming requests are routed through the HTTP cache, which implements most of the complexities of HTTP caching semantics and relevant RFC specifications1.</p> <p>For HTTP Requests:</p> <ul> <li>Considers the Cache-Control header of the request. For example, if the request has <code>Cache-Control: no-store</code>, the request is not cached.</li> <li>Does not store HTTP HEAD requests.</li> </ul> <p>For HTTP Responses:</p> <ul> <li>Only caches responses with enough data to calculate the freshness lifetime2.</li> <li>Considers the Cache-Control header from the upstream host. For example, if the HTTP response returns with status code <code>200</code> and <code>Cache-Control: max-age=60</code>, the response will be cached.</li> <li>Only caches responses with following status codes:<code>200, 203, 204, 206, 300, 301, 308, 404, 405, 410, 414, 451, 501</code>.</li> </ul> <p>The actual storage of HTTP responses is delegated to the implementations of a caching provider. These implementations can cover different requirements such as persistence, performance, and distribution, from local RAM caches to globally distributed persistent caches. They can be fully custom caches or wrappers and adapters for local or remote open source or proprietary caches. Currently, the available cache storage implementations are In-memory and Redis. More informations and relevant configurations of caching providers can be foud here.</p>"},{"location":"reference/cache/#configuration","title":"Configuration","text":"<p>The following example configures the HTTP cache with a default TTL (time-to-live) of 1200s and activates a custom debug header (X-Header) attached to every response that is served from Kache. Debug headers are added to the request and contain information if the response was served from cache or not. They are presented in a canonical format where <code>x_header_name</code> specifies the name of the header entry with its value indicating, if it was a cache <code>HIT</code> or <code>MISS</code>.</p> YAML <pre><code>cache:\n# Activate debug header.\nx_header: true\n# Set debug header name to 'X-Kache'.\nx_header_name: x-kache\n# Set default ttl for single items.\ndefault_ttl: 1200s\n</code></pre>"},{"location":"reference/cache/#reference","title":"Reference","text":"Directive Type Description <code>x_header</code> <code>bool</code> Activates the X-Cache debug header. If set to <code>true</code>, Kache will add a HTTP response header to each response indicating if it was served from cache or not (cache hit or miss). <code>x_header_name</code> <code>string</code> Specifies the name of the X-Cache debug header. For example, if set to <code>x-kache</code> and in case of a cache hit, the response will contain an additional HTTP header with <code>X-Kache</code> as key and <code>HIT</code> as value. Default is 'X-Kache'. <code>default_ttl</code> <code>string</code> Is the default TTL (time-to-live) for cached items. The value is a duration string. A duration string is a possibly signed sequence of decimal numbers, each with optional fraction and a unit suffix, such as \"300ms\", \"1.5h\", or \"2h45m\". Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\". TTL must be greater than 0. <ol> <li> <p>RFC7234 -- HTTP Caching \u21a9</p> </li> <li> <p>RFC7234 -- Calculating freshness lifetime \u21a9</p> </li> </ol>"},{"location":"reference/listeners/","title":"Listeners","text":"<p>A listener is a named network location (e.g., port, etc.) that can be connected to by downstream clients. Kache exposes one or more listeners that downstream hosts can connect to.</p> <p>A listener is configured with a unique name and a network adress. The address defines a port, and optionally a hostname, on which kache listens for incoming connections.</p>"},{"location":"reference/listeners/#configuration","title":"Configuration","text":"<p>For example, the following configuration exposes two named listeners, <code>web1</code> and <code>web2</code> listening on ports <code>:80</code> and <code>:1337</code>, respectively.</p> YAML <pre><code>listeners:\n# Listener 1 named web1\nweb1:\naddr: :80\n# Listener 2 named web2\nweb2:\naddr: :1337\n</code></pre> <p>After the listeners are up and running, kache accepts connections and forwards them to defined upstream targets.</p>"},{"location":"reference/listeners/#reference","title":"Reference","text":"Directive Type Description <code>&lt;name&gt;</code> <code>token</code> Name variable is the unique name of the listener. <code>addr</code> <code>string</code> The network location exposed by Kache that can be connected to by downstream clients. It accepts a port, and optionally a hostname, in the format of <code>[host]:port</code>."},{"location":"reference/targets/","title":"Targets","text":"<p>Targets are a group of logically similar upstream hosts that Kache connects to. An upstream host receives connections and requests from Kache and returns responses.</p>"},{"location":"reference/targets/#configuration","title":"Configuration","text":"<p>The following example defines two upstream hosts identified by a unique name. The upstream target's address (<code>addr</code>) is a network location, Kache can connect and send requests to.</p> YAML <pre><code>upstreams:\n# Upstream service 1\n- name: service1\naddr: \"&lt;ip-service-1&gt;:&lt;port-service-1&gt;\"\npath: \"path/to/service/1\"\n# Upstream service 2\n- name: service2\naddr: \"&lt;ip-service-2&gt;:&lt;port-service-2&gt;\"\n</code></pre>"},{"location":"reference/targets/#reference","title":"Reference","text":"Directive Type Description <code>name</code> <code>string</code> The unique identifier of the upstream target. <code>addr</code> <code>string</code> The network location Kache can connect and send requests to. Typically a valid URL consisting of the private IP and Port of an upstream service. <code>path</code> <code>string</code> The path prefix used to match the upstream target. Path is forwarded as-is, hence the service is expected to listen on the specified path."},{"location":"setup/","title":"Setup","text":""}]}