{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>kache is a modern cloud-native web accelerator and HTTP caching proxy that is highly available, reliable, and performant. It supports the latest RFC specifications and is capable of handling high traffic loads, scaling easily, and supporting distributed caching systems.</p>"},{"location":"getting-started/","title":"Getting started","text":""},{"location":"license/","title":"License","text":"<p>MIT License</p> <p>Copyright (c) 2023 kache.io</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"support/","title":"Sponsors","text":"<p>kache is sponsored and supported by Media Tech Lab.</p> <p> </p>"},{"location":"blog/","title":"Blog","text":""},{"location":"intro/configuration/","title":"Configuration","text":"<p>Kache can be easily configured with a YAML-based configuration file. </p> <p>The configuration contains everything that defines how HTTP requests and responses are handled and cached. Elements of the configuration include defining and configuring listeners as endpoints to which downstream clients can connect and upstream target hosts to which Kache connects.</p> <p>The HTTP caching element contains the configuration options for the HTTP caching filter or middleware that sits between listeners and upstream targets, and the provider element that configures the actual storage backend.</p> <p>Other elements allow to configure operations such as logging and exposing the Admin API.</p> <p>For a full overview, check the configuration reference or the sample configuration file on GitHub:</p> <ul> <li>Reference </li> <li>Sample config</li> </ul>"},{"location":"intro/configuration/#loading-the-configuration","title":"Loading the configuration","text":"<p>The configuration file is loaded when the service is started. The path to the configuration file must be specified via the <code>config.file</code> command line flag:</p> <pre><code>./kache -config.file=kache.yml\n</code></pre> <p>While the service is running, the configuration values can be changed temporarily or permanently. Currently, only changes affecting the HTTP cache can be applied at runtime and without completely restarting the service.</p>"},{"location":"intro/configuration/#changing-the-configuration","title":"Changing the configuration","text":""},{"location":"intro/configuration/#temporary","title":"Temporary","text":"<p>Before applying permanent changes, it is advisable to test the new configuration before carefully rolling it out to production. Changing configuration parameters using the Cache Management API is an excellent way to test configuration changes before applying them permanently. Items and values that are changed in this way do not survive a restart of the service.</p>"},{"location":"intro/configuration/#permanent","title":"Permanent","text":"<p>In order to make and apply permanent changes to the service's configuration, the corresponding file that was loaded when the service was started must be edited and reloaded. Reloading the configuration can be done in two ways: either by explicit reloading, sending a SIGHUP to the running process, or by enabling automatic reloading when Kache is started.</p>"},{"location":"intro/configuration/#reloading-the-configuration","title":"Reloading the configuration","text":"<p>To apply configuration changes to the configuration file without completely restarting the running service, Kache listens for the SIGHUP signal. Sending the signal to the running process triggers the reloading of the configuration file. The new configuration is applied only if something has changed.</p> <pre><code>kill -SIGHUP &lt;pid&gt;\n</code></pre>"},{"location":"intro/configuration/#auto-reloading","title":"Auto-reloading","text":"<p>It is possible to automatically reload the configuration file if it has changed. Automatic reloading can be enabled at service startup via the <code>--config.auto-reload</code> flag . If enabled, Kache will watch the specified configuration file for changes and automatically reloads if it has changed. The watch interval can be specified with the <code>--config.watch-interval=&lt;duration&gt;</code> flag, where <code>&lt;duration&gt;</code> must be a valid duration string, e.g. <code>5s</code>. If auto-reload is enabled, but no watch interval is specified, the default interval is <code>10s</code>.</p> <pre><code>./kache --config.file=kache.yml --config.auto-reload --config.watch-interval=1s\n</code></pre>"},{"location":"intro/installation/","title":"Installation","text":"<p>You can install and run Kache in the following ways:</p> <ul> <li>Use the official Docker image</li> <li>Use the binary distribution</li> <li>Build binary from source</li> <li>Use the Helm Chart</li> </ul>"},{"location":"intro/installation/#use-the-official-docker-image","title":"Use the Official Docker Image","text":"<p>Use one of the official Docker images and run it with the sample configuration file:</p> <pre><code>docker run -d -p 8080:8080 -p 80:80 \\\n    -v $PWD/kache.yml:/etc/kache/kache.yml \\\n    kache:latest -config.file=/etc/kache/kache.yml \n</code></pre>"},{"location":"intro/installation/#use-the-binary-distribution","title":"Use the binary distribution","text":"<p>To run kache, get the latest binary from the releases page and run it with the sample configuration file:</p> <pre><code>./kache -config.file=kache.yml\n</code></pre>"},{"location":"intro/installation/#build-binary-from-source","title":"Build binary from source","text":"<pre><code>git clone https://github.com/kacheio/kache\n</code></pre>"},{"location":"intro/installation/#use-the-helm-chart","title":"Use the Helm Chart","text":"<p>Info</p> <p>Comming soon!</p>"},{"location":"intro/installation/#quick-start","title":"Quick Start","text":"<p>If you want to run kache with a distributed caching backend (e.g. Redis), you can use and run this example docker-compose as a starting point:</p> <pre><code>docker-compose -f cloud/docker-compose.yml up \n</code></pre> <p>Tip</p> <p>Check the Quick Starts for Docker and Kubernetes to learn more.</p>"},{"location":"intro/quick-start-k8s/","title":"Kache on Kubernetes","text":"<p>The following describes how to run kache on a local Kubernetes cluster.</p> <p>Warning</p> <p>Please note that this is not intended for use in a production environment. We will provide more sophisticated configurations for operations in the future. </p>"},{"location":"intro/quick-start-k8s/#start-a-kubernetes-cluster","title":"Start a Kubernetes cluster","text":"<p>To start a cluster, use minikube or Docker Desktop.</p> <pre><code>minikube start\n</code></pre>"},{"location":"intro/quick-start-k8s/#deploy-configmap","title":"Deploy ConfigMap","text":"<p>Create a ConfigMap that contains the kache configuration:</p> <pre><code>kubectl create configmap kache-config --from-file=cloud/kubernetes/configmap.yml </code></pre> <p>Apply the ConfigMap:</p> <pre><code>kubectl apply -f cloud/kubernetes/configmap.yml\n</code></pre> configmap.yml Kubernetes <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\nname: kache-config\ndata:\nconfig.yml: |-\nlisteners:\nweb1:\naddr: :80\nweb2:\naddr: :1337\nupstreams:\n- name: service1\naddr: \"http://localhost:8000\"\npath: \"/service/1\"\n- name: service2\naddr: \"http://example.com\"\npath: \"/\"\napi:\nport: 1338\ndebug: true\nlogging:\nlevel: debug\nprovider:\nbackend: redis\nredis:\nendpoint: \"redis-master:6379\"\nusername:\npassword:\ndb:\n</code></pre>"},{"location":"intro/quick-start-k8s/#deploy-redis","title":"Deploy Redis","text":"<pre><code>kubectl apply -f cloud/kubernetes/redis-master.yml\n</code></pre> redis.yml Kubernetes <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: redis-master\nlabels:\napp: redis\nspec:\nselector:\nmatchLabels:\napp: redis\nrole: master\ntier: backend\nreplicas: 1\ntemplate:\nmetadata:\nlabels:\napp: redis\nrole: master\ntier: backend\nspec:\ncontainers:\n- name: master\nimage: redis\nresources:\nrequests:\ncpu: 100m\nmemory: 100Mi\nports:\n- containerPort: 6379\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: redis-master\nlabels:\napp: redis\nrole: master\ntier: backend\nspec:\nports:\n- port: 6379\ntargetPort: 6379\nselector:\napp: redis\nrole: master\ntier: backend\n</code></pre>"},{"location":"intro/quick-start-k8s/#deploy-kache","title":"Deploy Kache","text":"<pre><code>kubectl apply -f cloud/kubernetes/kache.yml\n</code></pre> kache.yml Kubernetes <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: kache\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp: kache\ntemplate:\nmetadata:\nlabels:\napp: kache\nspec:\ncontainers:\n- name: kache\nimage: kacheio/kache:main\nimagePullPolicy: Always\nargs:\n- \"-config.file=/etc/kache/config.yml\"\nenv:\n- name: NAMESPACE\nvalueFrom:\nfieldRef:\nfieldPath: metadata.namespace\nvolumeMounts:\n- name: config\nmountPath: /etc/kache\nports:\n- containerPort: 8080\nname: http\n- containerPort: 1337\nname: web\n- containerPort: 1338\nname: api\nresources:\nrequests:\ncpu: 100m\nmemory: 100Mi\nvolumes:\n- name: config\nconfigMap:\nname: kache-config\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: kache-service\nlabels:\napp: kache\nspec:\ntype: LoadBalancer\nports:\n- name: \"http\"\nport: 80\ntargetPort: http\n- name: \"web\"\nport: 1337\ntargetPort: web\n- name: \"api\"\nport: 1338\ntargetPort: api\nselector:\napp: kache\n</code></pre>"},{"location":"intro/quick-start-k8s/#accessing-the-service","title":"Accessing the service","text":"<p>Check that the Pods are up and running:</p> <pre><code>$ kubectl get pods NAME                           READY   STATUS    RESTARTS   AGE\nkache-54cd8ffd96-xzdqg         1/1     Running   0          14h\nredis-master-d4f785667-mpmvg   1/1     Running   0          14h\n</code></pre> <p>The Kache service is exposed as a LoadBalancer via the service with mapped ports and is accessible on localhost.</p> <pre><code>$ kubectl get svc\n\nNAME            TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                                      AGE\nkache-service   LoadBalancer   10.110.92.73   localhost     80:30135/TCP,1337:32284/TCP,1338:30691/TCP   44h\nkubernetes      ClusterIP      10.96.0.1      &lt;none&gt;        443/TCP                                      44h\nredis-master    ClusterIP      10.97.188.34   &lt;none&gt;        6379/TCP                                     44h\n</code></pre> <p>Use the above endpoints to access the service:</p> <pre><code>curl http://127.0.0.1:1337/\n</code></pre> <p>Access the API:</p> <pre><code>curl http://127.0.0.1:1338/api/\n</code></pre>"},{"location":"intro/quick-start-k8s/#troubleshooting","title":"Troubleshooting","text":"<p>If there are problems loading the configuration, verify that the Pod has the latest configuration available:</p> <pre><code>kubectl exec $POD_NAME -- cat /etc/kache/config.yml </code></pre>"},{"location":"intro/quick-start/","title":"Quick Start","text":"<p>The following describes how to run kache locally with Docker.</p>"},{"location":"intro/quick-start/#set-up-a-docker-compose","title":"Set up a docker-compose","text":"<p>Create and define a docker-compose.yml and define a kache service that uses the official Kache image:</p> <pre><code>services:\nkache:\n# Use the `main` tag for the latest development image \n# or `latest` tag for the latest stable version.\nimage: kacheio/kache:main\ncontainer_name: kache\n# Start the container with the mounted config file.\ncommand:\n- \"-config.file=/etc/kache/kache.sample.yml\"\n# Expose ports that are configured in the kache config file.\nports:\n- \"80:80\"\n- \"8080:8080\"\n- \"1337:1337\"\n- \"1338:1338\"\n# Mount the config file.\nvolumes:\n- \"./../kache.sample.yml:/etc/kache/kache.sample.yml\"\n# Use redis as distributed remote caching backend.\nredis:\nimage: \"redis:alpine\"\n</code></pre> <p>That's all you need to run Kache with Redis as a distributed caching backend. </p>"},{"location":"intro/quick-start/#run-kache","title":"Run kache","text":"<p>Now start Kache with the following command:</p> <pre><code>docker-compose -f cloud/docker/docker-compose.yml up \n</code></pre>"},{"location":"intro/quick-start/#access-kache","title":"Access kache","text":"<p>You can now access the service under <code>http://localhost:8080/</code> and the API under <code>http://localhost:1338/api/cache/keys</code> which, for example, returns all the keys currently in the cache.</p>"},{"location":"reference/","title":"Configuration reference","text":"<p>This section gives an overview of all relevant configuration options and provides further details on the core concepts of Kache.</p>"},{"location":"reference/api/","title":"API","text":"<p>Kache offers a REST API into Kache, exposing endpoints for cache management and debug information.</p> <p>For example, the following configuration exposes the API on the specified port and activates endpoints exposing debug informations.</p> YAML <pre><code>api:\n# API port.\nport: 1338\n# Activate debug endpoints.\ndebug: true\n</code></pre> <p>The Kache API endpoints are generally available under path prefix <code>/api</code>, unless not specified a different prefix. A complete overview of the API and its endpoints is available here.</p>"},{"location":"reference/api/#reference","title":"Reference","text":"Directive Type Description <code>port</code> <code>string</code> The port the API is available at. <code>prefix</code> <code>string</code> Adds a custom path prefix. <code>acl</code> <code>string</code> The access control list (ACL) is a comma-separated list of IP addresses granted to access the API. <code>debug</code> <code>bool</code> Activates debug endpoints."},{"location":"reference/api_specification/","title":"API","text":"<p>Kache provides an API for cache control and managment and exposes a number of informations through the API, such as the configuration of all listeners, targets, etc. The admin API can be enabled via the configuration.</p>"},{"location":"reference/api_specification/#security","title":"Security","text":"<p>Enabling the API will expose all configuration elements, including sensitive data.</p> <p>Kache supports the configuration of an access control list, which grants or denies access to API resources and objects. </p> <p>In production, it should be further protected by application level protection mechanisms, e.g., secured by authentication and authorization. Or, by transport level protection mechanisms such as NOT publicly exposing the API's port, but keeping it restricted to internal networks (principle of least privilege, applied to networks).</p>"},{"location":"reference/api_specification/#cache-management","title":"Cache Management","text":""},{"location":"reference/api_specification/#purging-a-cache-key","title":"Purging a cache key","text":"<p>Purging a key from the cache is done by isssuing a <code>PURGE</code> request to the API root available at the  specified API port. The cache key is retrieved from a custom request header <code>X-Purge-Key</code>. When specified in wildcard syntax, all matching keys will be purged from the cache. If the specified key is an empty string, all keys will be purged from the cache, similar to a cache flush.</p> <pre><code>curl -v -X PURGE -H 'X-Purge-Key: &lt;cache-key-pattern&gt;' kacheserver:$PORT\n</code></pre>"},{"location":"reference/api_specification/#flush-cache-keys","title":"Flush cache keys","text":"<p>Flushing the cache deletes all the keys from the cache:</p> <pre><code>curl -v -X DELETE 'kacheserver:$PORT/&lt;api-prefix&gt;/cache/flush'\n</code></pre>"},{"location":"reference/api_specification/#list-cache-keys","title":"List cache keys","text":"<p>To list all the keys in the cache:</p> <pre><code>curl -v -X GET 'kacheserver:$PORT/&lt;api-prefix&gt;/cache/keys'\n</code></pre>"},{"location":"reference/api_specification/#cache-configuration","title":"Cache configuration","text":"<p>The API provides endpoints to render and update the current cache configuration. Updating the cache configuration via the API currently only updates the runtime configuration and does not keep the changes in the specified configuration file that is loaded when kachserver is started. Thus, the changes are not preserved when the service is restarted.</p> <p>Rendering the configuration: <pre><code>curl -v -X GET 'kacheserver:$PORT/&lt;api-prefix&gt;/cache/config'\n</code></pre></p> <p>Updating the configuration:</p> Example: Update cache configuration cURL <pre><code>curl --location --request PUT 'kacheserver:$PORT/&lt;api-prefix&gt;/cache/config/update' \\\n--header 'Content-Type: application/json' \\\n--data '{\n        \"x_header\": true,\n        \"x_header_name\": \"x-kache\",\n        \"default_ttl\": \"1200s\",\n        \"default_cache_control\": \"max-age=120s\",\n        \"force_cache_control\": false,\n        \"timeouts\": [\n            {\n                \"path\": \"/archive\",\n                \"ttl\": 86400s\n            },\n            {\n                \"path\": \"^/assets/([a-z0-9].*).css\",\n                \"ttl\": 30s\n            }\n        ],\n        \"exclude\": {\n            \"path\": [\n                \"^/admin\",\n                \"^/test\",\n                \"^/.well-known/acme-challenge/(.*)\"\n            ],\n            \"header\": {\n                \"x_requested_with\": \"XMLHttpRequest\"\n            },\n            \"content\": [\n                {\n                    \"type\": \"application/javascript|text/css|image/.*\",\n                    \"size\": 10000\n                }\n            ]\n        }\n    }'\n</code></pre>"},{"location":"reference/api_specification/#configuration","title":"Configuration","text":""},{"location":"reference/api_specification/#port","title":"Port","text":"<p>This enables the API and exposes its endpoints via the specified port.</p> YAML <pre><code>api:\n# API port.\nport: 1338\n</code></pre>"},{"location":"reference/api_specification/#prefix","title":"Prefix","text":"<p>Configure a custom path prefix for all API endpoints, other than debug, version, and purge.</p> YAML <pre><code>api:\n# Enable API at port.\nport: 1338\n# Customize path prefix, default is '/api'.\nprefix: \"/custom-api-prefix/\"\n</code></pre>"},{"location":"reference/api_specification/#acl","title":"ACL","text":"<p>Configure an access control list to grant access to resources only for the specified IP addresses. If  <code>acl</code> is specified, access is granted only for requests originating from IPs on the list. All others  will be denied. If the list is empty or the item is not specified in the configuration at all,  any request is allowed.</p> YAML <pre><code>api:\n# Enable API at port.\nport: 1338\n# Access control list. If empty or not specified at all, \n# any request is allowed to access API resources and objects.\nacl: \"127.0.0.1, 10.22.0.0\"\n</code></pre>"},{"location":"reference/api_specification/#debug","title":"Debug","text":"<p>Activate endpoints for general debug informations.</p> YAML <pre><code>api:\n# API port.\nport: 1338\n# Activate debug endpoints.\ndebug: true\n</code></pre>"},{"location":"reference/api_specification/#endpoints","title":"Endpoints","text":"Path Method Description <code>/</code> <code>PURGE</code> Purges a <code>&lt;key&gt;</code> from the cache. The cache key is retrieved from a custom request header <code>X-Purge-Key</code> and support wildcard matching. <code>/&lt;api-prefix&gt;/cache/keys</code> <code>GET</code> Returns the keys currently in the cache. <code>/&lt;api-prefix&gt;/cache/keys/&lt;key&gt;</code> <code>GET</code> Returns the key info about <code>&lt;key&gt;</code>. <code>/&lt;api-prefix&gt;/cache/flush</code> <code>DELETE</code> Flushes all keys from the cache. <code>/&lt;api-prefix&gt;/cache/config</code> <code>GET</code> Renders the current cache config. <code>/&lt;api-prefix&gt;/cache/config/update</code> <code>PUT</code> Updates the current cache config. <code>/version</code> <code>GET</code> Returns the Kache version info. <code>/debug/vars</code> <code>GET</code> See the expvar Go documentation. <code>/debug/pprof</code> <code>GET</code> See the pprof Index Go documentation. <code>/debug/pprof/cmdline</code> <code>GET</code> See the pprof Cmdline Go documentation. <code>/debug/pprof/profile</code> <code>GET</code> See the pprof Profile Go documentation. <code>/debug/pprof/symbol</code> <code>GET</code> See the pprof Symbol Go documentation. <code>/debug/pprof/trace</code> <code>GET</code> See the pprof Trace Go documentation."},{"location":"reference/cache/","title":"HTTP Cache","text":"<p>The HTTP cache acts as a filter or middleware sitting between listeners and upstream targets. All incoming requests are routed through the HTTP cache, which implements most of the complexities of HTTP caching semantics and relevant RFC specifications1.</p> <p>For HTTP Requests:</p> <ul> <li>Considers the Cache-Control header of the request. For example, if the request has <code>Cache-Control: no-store</code>, the request is not cached.</li> <li>Does not store HTTP HEAD requests.</li> </ul> <p>For HTTP Responses:</p> <ul> <li>Only caches responses with enough data to calculate the freshness lifetime2.</li> <li>Considers the Cache-Control header from the upstream host. For example, if the HTTP response returns with status code <code>200</code> and <code>Cache-Control: max-age=60</code>, the response will be cached.</li> <li>Only caches responses with following status codes:<code>200, 203, 204, 206, 300, 301, 308, 404, 405, 410, 414, 451, 501</code>.</li> </ul> <p>The actual storage of HTTP responses is delegated to the implementations of a caching provider. These implementations can cover different requirements such as persistence, performance, and distribution, from local RAM caches to globally distributed persistent caches. They can be fully custom caches or wrappers and adapters for local or remote open source or proprietary caches. Currently, the available cache storage implementations are In-memory and Redis. More informations and relevant configurations of caching providers can be foud here.</p>"},{"location":"reference/cache/#configuration","title":"Configuration","text":"<p>Kache provides several options for customizing the caching behavior. These configurations can be changed either permanently via the configuration file or temporarily via the configuration API. See the Configuration reference and API reference for detailed explanations.</p>"},{"location":"reference/cache/#custom-headers","title":"Custom headers","text":"<p>Kache adds custom headers (X-Header) to each response that is served by Kache. These headers contain information about whether the response was delivered from the cache or not, and can be useful for debugging. </p> <p>The headers are disabled by default and must be enabled in the configuration. Once enabled, debug headers are added to each response and contain information about whether the response was delivered from the cache or not. </p> <p>They are presented in a canonical format, where <code>x_header_name</code> specifies the name of the corresponding header entry and its value indicating whether it is a cache <code>HIT</code> or <code>MISS</code>. If no header name is specified in the configuration, the default header name (<code>X-Kache</code>) is used.</p> YAML <pre><code>cache:\n# Activate debug header.\nx_header: true\n# Set debug header name to 'X-Kache'.\nx_header_name: x-kache\n</code></pre>"},{"location":"reference/cache/#cache-control","title":"Cache-Control","text":"<p>The HTTP header field Cache-Control contains directives (instructions) \u2013 both in requests and responses \u2013 that control caching in browsers and shared caches (e.g. proxies). The Cache-Control header specifies directives that must be followed by all caching mechanisms. </p> <p>Kache fully implements the latest RFC specification1 and respects these directives when caching responses to requests. However, in some cases it is desirable to customize directives sent by backends, or to add directives for responses that do not include a Cache-Control header at all. Therefore, Kache allows setting or modifying the Cache-Control header field of such upstream responses. It is, however, advisable to not change the Cache-Control values set by backend servers and to do so only when necessary, as this could lead to undesirable side effects.</p> <p>If <code>default_cache_control_header</code> is specified in the configuration, a Cache-Control header with the corresponding value will be added to any response that does not contain a Cache-Control header. If the backend sets a header, overriding it with the configured default header can be enforced via the <code>force_cache_control</code> directive.</p> <p>For example, the following configuration sets a custom Cache-Control header field with a <code>max-age</code> of 2 minutes (or 120 seconds) on each incoming upstream response and defines for how long the resource is cachable. Note that <code>max-age</code> is a relative time defined in seconds. If the upstream response already contains a Cache-Control header, it will be overwritten due to <code>force_cache_control</code> set to <code>true</code>.</p> YAML <pre><code>cache:\n# Set default cache-control header if missing or enforced to update.\ndefault_cache_control: \"max-age=120\"\n# Always set the specified default cache-control regardless if present or not.\nforce_cache_control: true\n</code></pre>"},{"location":"reference/cache/#expiration","title":"Expiration","text":"<p>The lifetime of a cached item is represented by its TTL (time-to-live). An item lives in the cache until its TTL elapses and the cached resource expires. After that time, the item is removed from the cache. Resources within their TTL are considered fresh, while stale resources are those whose lifetime has already expired. Setting the right lifetime durations is fundamental for a healthy cache, avoids consuming valuable system resources such as cache memory, and ensures that users have an optimal experience.</p> <p>For a HTTP cache that stores responses, the TTL can be set via HTTP header either by the origin servers or by Kache itself. If set by the origin server, Kache respects those values. Changing or overriding those values is still possible through the configuration. If no TTLs have been set by the origin server, Kache applies default and custom TTLs that can be defined per path and resource. </p> <p>For example, the following configuration configures the HTTP cache with a default TTL (time-to-live) of 1200s for each resource, but applies custom and more fine-grained TTLs to specific paths and resources.</p> YAML <pre><code>cache:\n# Default TTL in seconds.\ndefault_ttl: 1200s\n# Custom TTLs per path/resouce.\ntimeouts:\n- path: \"/news\"\nttl: \"10s\"\n- path: \"/archive\"\nttl: \"86400s\"\n- path: \"^/assets/([a-z0-9].*).css\"\nttl: \"120s\"\n</code></pre>"},{"location":"reference/cache/#ignore","title":"Ignore","text":"<p>Any resource is cached and stored as long as it is cacheable and fresh (has not expired).</p> <p>To explicitly ignore certain requests and responses, Kache provides the ability to exclude those resources from being cached. This can be done either for requests with a specific path, for requests with a specific header, or for responses containing objects of a specific content type and size.</p> <p>For example, the following configuration specifies that requests to <code>/admin</code> are bypassed by Kache and thus excluded from caching. The same is true for requests containing a request header with key <code>X-Requested-With</code> and value <code>XMLHttpRequest</code>. In addition, responses containing javascript, css, and images with a size greater than 1MB are not cached as well. </p> YAML <pre><code>cache:\n# Exclude resources from cache.\nexclude:\n# Exclude all requests matching the specified path (regex).\npath:\n- \"^/admin\" - \"^/.well-known/acme-challenge/(.*)\"\n# Exclude all request with a specific header field and value.\nheader:\nx_requested_with: \"XMLHttpRequest\"\n# Don't cache responses depending on their type and size.\ncontent: # applied to responses\n- type: \"text/javascript|text/css|image/.*\"\nsize: 1000000 # in bytes\n</code></pre>"},{"location":"reference/cache/#mode","title":"Mode","text":"<p>Kache can be operated with two different cache modes, which directly influence the caching behavior. The default cache mode is <code>strict</code>. In this mode, HTTP cache respects the directives set in the Cache-Control header and automatically takes care of the lifetime of cached items and their proper validation with the origin once they become stale or are no longer considered fresh. If disabled with <code>strict: false</code> (caceh mode all), the HTTP cache ignores the Cache-Control directives, skips any valdation based on the cache control header, and stores each response until its TTL (time-to-live) expires. The corresponding TTLs must be specified by <code>default_ttl</code> and <code>timeouts</code>, respectively, as further described here.</p>"},{"location":"reference/cache/#reference","title":"Reference","text":"Directive Type Description <code>x_header</code> <code>bool</code> Activates the X-Cache debug header. If set to <code>true</code>, Kache will add a HTTP response header to each response indicating if it was served from cache or not (cache hit or miss). <code>x_header_name</code> <code>string</code> Specifies the name of the X-Cache debug header. For example, if set to <code>x-kache</code> and in case of a cache hit, the response will contain an additional HTTP header with <code>X-Kache</code> as key and <code>HIT</code> as value. Default is 'X-Kache'. <code>strict</code> <code>bool</code> Toggles the cache mode. Default is strict (<code>strict: true</code>). When set to <code>false</code>, the HTTP cache ignores the Cache-Control directives, skips any valdation based on the cache control header, and stores each response until its TTL (time-to-live) expires. <code>default_ttl</code> <code>string</code> Is the default TTL (time-to-live) for cached items. The value is a duration string. A duration string is a possibly signed sequence of decimal numbers, each with optional fraction and a unit suffix, such as \"300ms\", \"1.5h\", or \"2h45m\". Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\". TTL must be greater than 0. <code>default_cache_control</code> <code>string</code> Specifies a custom Cache-Control header. If configured, the custom Cache-Control header is added to each response that does not contain a valid Cache-Control header. The value must be a valid directive according to the corresponding RFC specification. <code>force_cache_control</code> <code>bool</code> Specifies whether to overwrite and modify an existing Cache-Control header. If set to <code>true</code>, the Cache-Control header set by the origin server will be overwritten with the value set in <code>default_cache_control</code>. <code>timeouts</code> <code>list</code> Configuration section for custom timeouts (list of TTLs per path). <code>path</code> <code>string</code> The path to the resource the TTL is applied to. Accepts a string or a valid regex. <code>ttl</code> <code>string</code> The duration after the specified resource expires. Accepts a valid duration string. <code>excluded</code> <code>list</code> Configuration section for resources excluded from cache. <code>path</code> <code>string</code> The request path that will be ignored. Accepts a string or a valid regex. <code>header</code> <code>string</code> Headers to be ignored by the cache. Header names must be specified in snake case format which is then resolved to a canonical format. E.g. to match the canonical header name X-Requested-With, the corresponding value in the configuration must be <code>x_requested_with</code>. <code>content</code> <code>list</code> Configuration section containing the content ignored by the cache. List of type and size (size is optional). <code>type</code> <code>string</code> Type is the content type to be ignored by the cache. Accepts a string or a valid regex. See here for an up-to-date and complete list of media types. <code>size</code> <code>int</code> Size is the max content size in bytes. For the every corresponding matching content type in <code>type</code>, resources that exceed the specified size are not cached. <ol> <li> <p>RFC7234 -- HTTP Caching \u21a9\u21a9</p> </li> <li> <p>RFC7234 -- Calculating freshness lifetime \u21a9</p> </li> </ol>"},{"location":"reference/file/","title":"File","text":"YAML <pre><code>listeners:\nweb1:\naddr: :80\nweb2:\naddr: :1337\nupstreams:\n- name: service1\naddr: \"http://localhost:8000\"\npath: \"/service/1\"\n- name: service2\naddr: \"http://example.com\"\napi:\nport: 1338\ndebug: true\nlogging:\nlevel: debug # trace, debug, info, warn, error, fatal, panic\ncolor: true\ncache:\nx_header: true\nx_header_name: x-kache\ndefault_ttl: 1200s\nprovider:\nbackend: redis\nlayered: true\nredis:\nendpoint: \"redis:6379\"\nusername:\npassword:\ndb:\nmax_item_size: 10000000\nmax_queue_concurrency: 56\nmax_queue_buffer_size: 24000\ninmemory:\nmax_size: 1000000000 max_item_size: 50000000\ndefault_ttl: 120s\n</code></pre>"},{"location":"reference/listeners/","title":"Listeners","text":"<p>A listener is a named network location (e.g., port, etc.) that can be connected to by downstream clients. Kache exposes one or more listeners that downstream hosts can connect to.</p> <p>A listener is configured with a unique name and a network adress. The address defines a port, and optionally a hostname, on which kache listens for incoming connections.</p>"},{"location":"reference/listeners/#configuration","title":"Configuration","text":"<p>For example, the following configuration exposes two named listeners, <code>web1</code> and <code>web2</code> listening on ports <code>:80</code> and <code>:1337</code>, respectively.</p> YAML <pre><code>listeners:\n# Listener 1 named web1\nweb1:\naddr: :80\n# Listener 2 named web2\nweb2:\naddr: :1337\n</code></pre> <p>After the listeners are up and running, kache accepts connections and forwards them to defined upstream targets.</p>"},{"location":"reference/listeners/#reference","title":"Reference","text":"Directive Type Description <code>&lt;name&gt;</code> <code>token</code> Name variable is the unique name of the listener. <code>addr</code> <code>string</code> The network location exposed by Kache that can be connected to by downstream clients. It accepts a port, and optionally a hostname, in the format of <code>[host]:port</code>."},{"location":"reference/logging/","title":"Logging","text":"<p>Kache supports efficient structured logging with different output locations and formats:</p> <ul> <li>Console</li> <li>Log file</li> <li>JSON</li> </ul>"},{"location":"reference/logging/#console","title":"Console","text":"<p>Pretty logging to the console is supported and activated by default.</p> YAML <pre><code>logging:\n# Set level to debug.\nlevel: debug\n# Activate color log output.\ncolor: true # disabled by default\n</code></pre>"},{"location":"reference/logging/#file","title":"File","text":"<p>To use structured file-based logging, just configure a log location.</p> YAML <pre><code>logging:\nlevel: info\n# Activate file-based logging.\nfile: /var/log/kache/kache.log\n# Configure file-based logging.\nmax_size:    500 # 500 megabytes\nmax_backups: 3 max_age:     28 # days\n</code></pre> Info <p>Kache opens or creates the logfile on first write. If the file exists and is less than MaxSize megabytes, it will open and append to that file. If the file exists and its size is &gt;= MaxSize megabytes, the file is renamed by putting the current time in a timestamp in the name immediately before the file's extension (or the end of the filename if there's no extension). A new log file is then created using original filename.</p> <p>Whenever a write would cause the current log file exceed MaxSize megabytes, the current file is closed, renamed, and a new log file created with the original name. Thus, the filename you give Logger is always the \"current\" log file.</p> <p>Backups use the log file name given to Logger, in the form name-timestamp.ext where name is the filename without the extension, timestamp is the time at which the log was rotated formatted with the time.Time format of 2006-01-02T15-04-05.000 and the extension is the original extension. For example, if your Logger.Filename is /var/log/foo/server.log, a backup created at 6:30pm on Nov 11 2016 would use the filename /var/log/foo/server-2016-11-04T18-30-00.000.log</p> <p>More..</p>"},{"location":"reference/logging/#json","title":"JSON","text":"<p>To log in JSON format, just change the format of the log output to <code>json</code>. This works with both, console and file logging.</p> YAML <pre><code>logging:\n# Set level to debug.\nlevel: debug\n# Set format to json.\nformat: json\n</code></pre>"},{"location":"reference/logging/#reference","title":"Reference","text":"Directive Type Description <code>level</code> <code>string</code> Specifies the log level. Supported log levels are: <code>trace</code>, <code>debug</code>, <code>info</code>, <code>warn</code>, <code>error</code>, <code>fatal</code>, <code>panic</code>. <code>format</code> <code>string</code> Specifies the log format. Default is console, <code>json</code> changes log output to JSON format. <code>color</code> <code>bool</code> Enables colored log output. Disabled by default. Logging to files never colors the output. <code>file</code> <code>string</code> Sets the log file path. <code>max_size</code> <code>int</code> Sets the maximum size in megabytes of the log file before it gets rotated. It defaults to 100 megabytes. <code>max_age</code> <code>int</code> Sets the maximum number of days to retain old log files based on the timestamp encoded in their filename.  Note that a day is defined as 24 hours and may not exactly correspond to calendar days due to daylight savings, leap seconds, etc. The default is not to remove old log files based on age. <code>max_backups</code> <code>int</code> Sets the maximum number of old log files to retain.  The default `s to retain all old log files (though MaxAge may still cause them to get deleted.) <code>compress</code> <code>bool</code> Specifies if the rotated log files should be compressed using gzip. The default is not to perform compression."},{"location":"reference/provider/","title":"Provider","text":"<p>A provider is responsible for the actual storage of HTTP responses. A provider implementation can satisfy different requirements such as persistence, performance, and distribution, from local RAM caches to globally distributed persistent caches. They can be fully custom caches or wrappers and adapters for local or remote open-source or proprietary caches. </p> <p>At the moment, the available and supported cache storage implementations are In-memory and Redis.</p> <ul> <li>In-memory</li> <li>Remote</li> </ul>"},{"location":"reference/provider/#in-memory-cache","title":"In-Memory cache","text":"<p>In-memory stores HTTP responses in a local in-memory cache. It is a volatile storage and does not persist the entries to disk. It is also not shared between multiple Kache instances. The in-memory cache is size-bound and ensures the total cache size approximately does not exceed the configured <code>max_size</code> in bytes. The following shows an example configuration of an in-memory cache with maximum overall cache size and maximum single item size constraints and with TTL eviction enabled and set to 120s.</p> YAML <pre><code>provider:\n# Activate inmemory as cache backend.\nbackend: inmemory\n# Configure inmemory cache.\ninmemory:\n# Overall cache size of 1GB.\nmax_size: 1000000000\n# Max item size of 50MB.\nmax_item_size: 50000000\n# Items expire after 120s.\ndefault_ttl: 120s\n</code></pre> <p>Tip</p> <p>To disable TTL eviction, set <code>default_ttl</code> to -1.</p>"},{"location":"reference/provider/#remote-cache","title":"Remote cache","text":"<p>A remote cache is a distributed remote cache such as Redis or Memcached. The following example configures Redis as a remote caching provider.</p> YAML <pre><code>provider:\n# Activate redis as cache backend.\nbackend: redis\n# Configure redis remote cache.\nredis:\nendpoint: \"redis:6379\" username:\npassword:\ndb: max_item_size: 50000000\nmax_queue_concurrency: 56\nmax_queue_buffer_size: 24000\n</code></pre>"},{"location":"reference/provider/#layered-cache","title":"Layered cache","text":"<p>A layered or tiered cache adds a local in-memory caching layer on top of a remote cache provider (typically a distributed and shared network cache). Items will always be stored in both caches. Fetches are only satified by the underlying remote cache, if the item does not exist in the local cache. The local cache will remove items, depending on the capacity constraints of the cache or the lifetime constraints of the cached item, respectively. This strategy can be used to speed up operations when working with remote network caches.</p> <p>The following example configures a remote cache that is layered by a local in-memory cache.</p> YAML <pre><code>provider:\n# Activate redis as the main remote cache backend.\nbackend: redis\n# Activate layered caching strategy.\nlayered: true\n// Configure remote redis cache (layer 2).\nredis:\nendpoint: \"redis:6379\"\nusername:\npassword:\ndb:\nmax_item_size: 50000000\nmax_queue_concurrency: 56\nmax_queue_buffer_size: 24000\n// Configure local inmemory cache (layer 1).\ninmemory:\nmax_size: 1000000000\nmax_item_size: 50000000\ndefault_ttl: 120s\n</code></pre>"},{"location":"reference/provider/#reference","title":"Reference","text":""},{"location":"reference/provider/#general","title":"General","text":"Directive Type Description <code>backend</code> <code>string</code> Specifies the backend used as the underlying cache storage provider. Valid values are <code>inmemory</code> for a local in-memory cache and <code>redis</code> for the distributed remote cache based on Redis. <code>layered</code> <code>bool</code> Activates a two-tier cache strategy, putting a local cache layer on top of the remote cache to speed up operations."},{"location":"reference/provider/#in-memory-cache_1","title":"In-Memory cache","text":"Directive Type Description <code>max_size</code> <code>int</code> Sets the overall maximum number of bytes the cache can hold. <code>max_item_size</code> <code>int</code> Sets the maximum size of a single item. <code>default_ttl</code> <code>string</code> Sets the defautl TTL of a single item. The value is a duration string. A duration string is a possibly signed sequence of decimal numbers, each with optional fraction and a unit suffix, such as \"300ms\", \"1.5h\", or \"2h45m\". Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\". A valid TTL must be greater than 0. If TTL is set to -1, TTL eviction is disabled."},{"location":"reference/provider/#redis-remote-cache","title":"Redis remote cache","text":"Directive Type Description <code>endpoint</code> <code>string</code> Specifies the endpoint addresses of the Redis server. Either a single address or a comma-separated list of host:port addresses of cluster/sentinel nodes. <code>username</code> <code>string</code> Sets the username to authenticate the current connection with one of the connections defined in the ACL list when connecting to a Redis 6.0 instance, or greater, that is using the Redis ACL system. <code>password</code> <code>string</code> Optional password. Must match the password specified in the requirepass server configuration option (if connecting to a Redis 5.0 instance, or lower), or the User Password when connecting to a Redis 6.0 instance, or greater, that is using the Redis ACL system. <code>db</code> <code>int</code> Sets the database to be selected after connecting to the server. Default is <code>0</code>. <code>max_item_size</code> <code>int</code> Specifies the maximum size of an item stored in Redis. Items bigger than MaxItemSize are skipped. If set to 0, no maximum size is enforced. <code>max_queue_concurrency</code> <code>int</code> Specifies the maximum number of enqueued job operations allowed. <code>max_queue_buffer_size</code> <code>int</code> Specifies the maximum number of concurrent asynchronous job operations."},{"location":"reference/targets/","title":"Targets","text":"<p>Targets are a group of logically similar upstream hosts that Kache connects to. An upstream host receives connections and requests from Kache and returns responses.</p>"},{"location":"reference/targets/#configuration","title":"Configuration","text":"<p>The following example defines two upstream hosts identified by a unique name. The upstream target's address (<code>addr</code>) is a network location, Kache can connect and send requests to.</p> YAML <pre><code>upstreams:\n# Upstream service 1\n- name: service1\naddr: \"&lt;ip-service-1&gt;:&lt;port-service-1&gt;\"\npath: \"path/to/service/1\"\n# Upstream service 2\n- name: service2\naddr: \"&lt;ip-service-2&gt;:&lt;port-service-2&gt;\"\n</code></pre>"},{"location":"reference/targets/#reference","title":"Reference","text":"Directive Type Description <code>name</code> <code>string</code> The unique identifier of the upstream target. <code>addr</code> <code>string</code> The network location Kache can connect and send requests to. Typically a valid URL consisting of the private IP and Port of an upstream service. <code>path</code> <code>string</code> The path prefix used to match the upstream target. Path is forwarded as-is, hence the service is expected to listen on the specified path."},{"location":"setup/","title":"Setup","text":""}]}